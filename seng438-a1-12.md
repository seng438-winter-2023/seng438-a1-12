>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group 12     |
|-----------------|
| Student 1 - Esohe Aideyan |   
| Student 2 - Jack Barrie   |   
| Student 3 - Tamunomiete Brown |   
| Student 4 - Dyenaan Dapoet|   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

During this lab, we carried out extensive tests on the ATM machine using different testing methods. The bug tracking system Jira was used to report bugs found during our testing. Pair testing was conducted beginning with exploratory tests where we tested every single function in the ATM system extensively and any defect found in the system was reported using Jira. Manual scripted Testing was also conducted using the SUT use cases provided. We made comparisons with our exploratory tests to see if more bugs could be detected. Regression testing was finally conducted where we verified the bugs with an updated version of the ATM system to determine if the defects were fixed, and any new defects were reported using Jira.

# High-level description of the exploratory testing plan

Our exploratory testing plan is to have the two pairs each test the system with a card number. First we will read over the system requirements and get familiar with its expected functionality. One group will have card #1 and the other card #2. Initially we will test functionality related to logging in. This includes invalid pin numbers and cards that don’t exist. Next we will use the atm system as a normal user would. We will test the withdrawals, keeping track of how much money the atm has for cash, and how much the bank account has. Then we will test the withdrawals for invalid inputs, like pressing keys that don't correspond to a withdrawal amount displayed on the system, or trying to withdraw from an account that the card doesn't have. Then we will test the deposits, keeping track of the account balance before and after a deposit. We will test deposits for all accounts displayed on the system, to make sure that we cant deposit money into an account that the card isn't linked to. Then we will test money transfers, ensuring that money isn't created or lost during a transfer. Finally we will test the balance inquiry, ensuring that the balance displayed by the system is correct. During these tests, we will test the cancel functionality at various points during a transaction. The log will be checked periodically after transactions are carried out to ensure that a correct history of transactions is maintained.

# Comparison of exploratory and manual functional testing

Exploratory testing is a form of testing in which the tester uses the system to find bugs without following a script. It allows the tester to test the application in any way they’d like. The goal of exploratory testing is to test the functionality of the system as a whole, rather than specific features. It requires the tester to understand the desired functionality of the system, which they use to compare to the actual functionality. Exploratory testing can catch bugs that wouldn't be found using scripted testing because of its open-ended nature. A disadvantage of exploratory testing is the tests are not consistent across test runs, so new versions can have bugs that go undetected.  Manual scripted testing has the tester following a predefined set of instructions to be carried out. Each test has an expected output, which is compared to the actual output of the test. Manual testing ensures consistency, which is beneficial when viewed from the perspective of applications life cycle. Scripted tests can be run on new versions of the system as they are released, which can certify that new releases meet functionality requirements. They are prone to human error, and take time to carry out, which is its main drawback. Both forms of testing have pros and cons, so a good approach is to use a mix of both tests for a more rigorous test of a system.

# Notes and discussion of the peer reviews of defect reports

We observed quite a number of defects while running and testing Version 1.0 of the simulation, some of which were fixed in Version 1.1. We also observed that most faults were common between the two cards. Finally, we came to a conclusion that although Version 1.1 had some faults that weren't fixed, it was still a better and more efficient version than Version 1.0.

# How the pair testing was managed and team work/effort was divided 

For the pair testing, we divided ourselves into two pairs: Pair 1, which is Esohe and Dyenaan, performed the tests for Card 1 and Pair 2, which is Tamunomiete and Jack, performed the tests for Card 2. Within each pair, the functionalities of the ATM system was further split into two and one person in a pair performed tests on the first two transaction types (Withdraw and Deposit) while the other person performed tests on the latter two (Transfer and Balance Inquiry). At the end of each testing, we switched functionalities in order to review and analyze what we did; first as a pair and then generally, as a group.


# Difficulties encountered, challenges overcome, and lessons learned

One of the major difficulties we encountered was gaining familiarity with the issue tracking software. As none of the members of the group was familiar with using the issue tracking software, it was a bit rocky at first understanding how to input the issues/bugs and assign them under the appropriate tags. To overcome this challenge, we all spent substantial amount of time, reporting issues and assigning them the appropriate tags and comments to get a feel of various features and functionality of the system. We also performed some searches to google to further explain the uses of some certain features. What we learned from this difficulty was that there would also be a learning opportunity in every experience and not to be afraid to reach out for help. Because if we had all stayed silent, and not admitted to fully understand the functionality of the software, it would have taken longer to complete the project/assignment.

Another difficulty we encountered during this assignment was conflicting schedules. As each memeber is taking and engaging in different courses as well as other activities, it was a bit of a challenge finding a suitable time for everyone. To first overcome this challenge, we were all very communicative about what worked and what did not, doing that helped to streamline times that worked for everyone. We also utilized applications like discord, google docs, github and text messaging to communicate and provide feedback and solutions. We learnt that we did not neccessarily have to meet in-person to get work done and that also helped to simulate situations that may occur in our future careers.

# Comments/feedback on the lab and lab document itself

Working on this lab gave us insight into software testing as we were able to practically apply all we had learnt during lectures. Figuring out the bug tracking system and how to do the pair testing was challenging at first but working together made it easier and that way, we were able to learn more about collaboration on projects which would be very useful in industry.
